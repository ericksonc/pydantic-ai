# Streaming with agent.iter: A Complete Guide

## Prerequisites

This guide assumes you're already familiar with:
- Using `agent.run()` for basic agent interactions
- Using `agent.run_stream()` for streaming responses (at a high level)
- The Vercel AI SDK Stream Protocol (see `claude/vercel_stream_protocol.md`)

This guide explains how to use `agent.iter()` for streaming, which gives you fine-grained control over the streaming process and works seamlessly with tools.

## Why agent.iter?

`agent.iter()` is the recommended approach for streaming because:

1. **Works with tools**: Unlike `run_stream().stream_text()`, it doesn't break when your agent has tools
2. **Fine-grained control**: Access to individual streaming events (text deltas, tool call deltas, thinking deltas)
3. **Graph-based execution**: You can see each step of the agent's execution as it happens
4. **Full compatibility**: Works with all agent features including tool calls, structured output, and thinking

## The Three Types of Streaming Deltas

When using `agent.iter()`, you'll encounter three types of deltas:

### 1. TextPartDelta - Regular Text Streaming

**What it is**: Incremental chunks of text content being generated by the model.

**Structure**:
```python
@dataclass
class TextPartDelta:
    content_delta: str              # The incremental text to append
    part_delta_kind: Literal['text']
```

**Vercel Protocol Mapping**:
```
TextPartDelta("Hello") → data: {"type":"text-delta","id":"msg_123","delta":"Hello"}
```

**Example**:
```python
from pydantic_ai.messages import PartDeltaEvent, TextPartDelta

async with agent.iter(prompt) as agent_run:
    async for node in agent_run:
        if Agent.is_model_request_node(node):
            async with node.stream(agent_run.ctx) as stream:
                async for event in stream:
                    if isinstance(event, PartDeltaEvent):
                        if isinstance(event.delta, TextPartDelta):
                            text_chunk = event.delta.content_delta
                            print(text_chunk, end='', flush=True)
```

### 2. ToolCallPartDelta - Tool Call Streaming

**What it is**: Incremental updates to a tool call as the model generates it (tool name, arguments, tool call ID).

**Structure**:
```python
@dataclass
class ToolCallPartDelta:
    tool_name_delta: str | None = None        # Incremental tool name text
    args_delta: str | dict[str, Any] | None = None  # Arguments (JSON string or dict)
    tool_call_id: str | None = None           # Tool call ID (not a delta, sets the ID)
    part_delta_kind: Literal['tool_call']
```

**How it works**:
- **tool_name_delta**: Streams character by character, e.g., `"get_"` → `"wea"` → `"ther"`
- **args_delta**: Can be either:
  - **JSON string**: Streamed incrementally, e.g., `'{"ci'` → `'ty":"'` → `'Paris"}'`
  - **Dict**: Merged incrementally, e.g., `{"city": "Paris"}` → `{"city": "Paris", "country": "France"}`
- **tool_call_id**: Set once when available (not streamed incrementally)

**Vercel Protocol Mapping**:
```
ToolCallPartDelta(tool_name_delta="get_weather")
  → data: {"type":"tool-input-start","toolCallId":"call_123","toolName":"get_weather"}

ToolCallPartDelta(args_delta='{"city":"')
  → data: {"type":"tool-input-delta","toolCallId":"call_123","inputTextDelta":"{\"city\":\""}

ToolCallPartDelta(args_delta='Paris"}')
  → data: {"type":"tool-input-delta","toolCallId":"call_123","inputTextDelta":"Paris\"}"}
  → data: {"type":"tool-input-available","toolCallId":"call_123","toolName":"get_weather","input":{"city":"Paris"}}
```

**Example**:
```python
from pydantic_ai.messages import PartDeltaEvent, ToolCallPartDelta, PartStartEvent, ToolCallPart

# Track accumulated tool calls by index
tool_calls: dict[int, ToolCallPart] = {}

async with agent.iter(prompt) as agent_run:
    async for node in agent_run:
        if Agent.is_model_request_node(node):
            async with node.stream(agent_run.ctx) as stream:
                async for event in stream:
                    if isinstance(event, PartStartEvent):
                        if isinstance(event.part, ToolCallPart):
                            # New tool call starting
                            tool_calls[event.index] = event.part
                            print(f"Tool call started: {event.part.tool_name}")

                    elif isinstance(event, PartDeltaEvent):
                        if isinstance(event.delta, ToolCallPartDelta):
                            # Update the tool call with the delta
                            if event.index in tool_calls:
                                tool_calls[event.index] = event.delta.apply(tool_calls[event.index])

                            # Stream the arguments as they arrive
                            if isinstance(event.delta.args_delta, str):
                                print(f"Tool args delta: {event.delta.args_delta}")
```

### 3. ThinkingPartDelta - Reasoning/Thinking Streaming

**What it is**: Incremental updates to the model's extended thinking/reasoning content (for models like Claude with extended thinking, OpenAI o1, etc.)

**Structure**:
```python
@dataclass
class ThinkingPartDelta:
    content_delta: str | None = None      # Incremental thinking content
    signature_delta: str | None = None    # Optional provider-specific signature
    provider_name: str | None = None      # Provider name for signature validation
    part_delta_kind: Literal['thinking']
```

**Supported by**:
- Anthropic Claude (extended thinking)
- OpenAI o1 (reasoning)
- Google Gemini (thought signatures)
- Amazon Bedrock

**Vercel Protocol Mapping**:
```
ThinkingPartDelta(content_delta="Let me analyze...")
  → data: {"type":"reasoning-start","id":"reasoning_123"}
  → data: {"type":"reasoning-delta","id":"reasoning_123","delta":"Let me analyze..."}
```

**Example**:
```python
from pydantic_ai.messages import PartDeltaEvent, ThinkingPartDelta, PartStartEvent, ThinkingPart

async with agent.iter(prompt) as agent_run:
    async for node in agent_run:
        if Agent.is_model_request_node(node):
            async with node.stream(agent_run.ctx) as stream:
                async for event in stream:
                    if isinstance(event, PartStartEvent):
                        if isinstance(event.part, ThinkingPart):
                            print("\n[Thinking started]")

                    elif isinstance(event, PartDeltaEvent):
                        if isinstance(event.delta, ThinkingPartDelta):
                            if event.delta.content_delta:
                                print(f"[Thinking] {event.delta.content_delta}", end='')
```

## Complete Event Flow Architecture

### Event Hierarchy

```
AgentStreamEvent
├── ModelResponseStreamEvent (from node.stream() on ModelRequestNode)
│   ├── PartStartEvent         - New part begins (text, tool call, or thinking)
│   ├── PartDeltaEvent         - Incremental update to a part
│   │   └── delta: ModelResponsePartDelta
│   │       ├── TextPartDelta
│   │       ├── ToolCallPartDelta
│   │       └── ThinkingPartDelta
│   └── FinalResultEvent       - Response matches output schema
│
└── HandleResponseEvent (from node.stream() on CallToolsNode)
    ├── FunctionToolCallEvent  - Tool is about to be called
    └── FunctionToolResultEvent - Tool execution completed
```

### Basic Streaming Pattern

```python
from pydantic_ai import Agent
from pydantic_ai.messages import (
    PartStartEvent, PartDeltaEvent, FinalResultEvent,
    TextPart, TextPartDelta,
    ToolCallPart, ToolCallPartDelta,
    ThinkingPart, ThinkingPartDelta,
    FunctionToolCallEvent, FunctionToolResultEvent
)

agent = Agent('openai:gpt-4o')

@agent.tool
def get_weather(city: str) -> str:
    return f"Sunny in {city}, 22°C"

async with agent.iter("What's the weather in Paris?") as agent_run:
    async for node in agent_run:
        # Model is generating a response
        if Agent.is_model_request_node(node):
            async with node.stream(agent_run.ctx) as stream:
                async for event in stream:
                    if isinstance(event, PartStartEvent):
                        print(f"\n[Part {event.index} started: {type(event.part).__name__}]")

                    elif isinstance(event, PartDeltaEvent):
                        if isinstance(event.delta, TextPartDelta):
                            print(event.delta.content_delta, end='', flush=True)
                        elif isinstance(event.delta, ToolCallPartDelta):
                            if event.delta.args_delta:
                                print(f"\n[Tool args: {event.delta.args_delta}]")
                        elif isinstance(event.delta, ThinkingPartDelta):
                            print(f"\n[Thinking: {event.delta.content_delta}]", end='')

                    elif isinstance(event, FinalResultEvent):
                        print("\n[Final result ready]")

        # Model is calling tools
        elif Agent.is_call_tools_node(node):
            async with node.stream(agent_run.ctx) as stream:
                async for event in stream:
                    if isinstance(event, FunctionToolCallEvent):
                        print(f"\n[Calling tool: {event.part.tool_name}]")
                    elif isinstance(event, FunctionToolResultEvent):
                        print(f"\n[Tool result: {event.result.content}]")
```

## Mapping to Vercel Stream Protocol

Here's how to convert Pydantic AI streaming events to Vercel SSE format:

```python
import json
from typing import AsyncIterator

async def stream_to_vercel_protocol(agent_run) -> AsyncIterator[str]:
    """Convert agent.iter() events to Vercel AI SDK stream protocol."""

    # Track active parts by index
    active_parts: dict[int, dict] = {}

    async for node in agent_run:
        if Agent.is_model_request_node(node):
            async with node.stream(agent_run.ctx) as stream:
                async for event in stream:

                    # Handle PartStartEvent
                    if isinstance(event, PartStartEvent):
                        part = event.part
                        idx = event.index

                        if isinstance(part, TextPart):
                            part_id = f"text_{idx}"
                            active_parts[idx] = {"id": part_id, "type": "text"}
                            # Text start
                            yield f'data: {json.dumps({"type": "text-start", "id": part_id})}\n\n'
                            if part.content:
                                yield f'data: {json.dumps({"type": "text-delta", "id": part_id, "delta": part.content})}\n\n'

                        elif isinstance(part, ToolCallPart):
                            tool_call_id = part.tool_call_id
                            active_parts[idx] = {"id": tool_call_id, "type": "tool"}
                            # Tool input start
                            yield f'data: {json.dumps({"type": "tool-input-start", "toolCallId": tool_call_id, "toolName": part.tool_name})}\n\n'
                            if part.args:
                                args_str = part.args_as_json_str()
                                yield f'data: {json.dumps({"type": "tool-input-delta", "toolCallId": tool_call_id, "inputTextDelta": args_str})}\n\n'

                        elif isinstance(part, ThinkingPart):
                            part_id = f"thinking_{idx}"
                            active_parts[idx] = {"id": part_id, "type": "thinking"}
                            # Reasoning start
                            yield f'data: {json.dumps({"type": "reasoning-start", "id": part_id})}\n\n'
                            if part.content:
                                yield f'data: {json.dumps({"type": "reasoning-delta", "id": part_id, "delta": part.content})}\n\n'

                    # Handle PartDeltaEvent
                    elif isinstance(event, PartDeltaEvent):
                        delta = event.delta
                        idx = event.index

                        if idx not in active_parts:
                            continue

                        part_info = active_parts[idx]

                        if isinstance(delta, TextPartDelta):
                            if delta.content_delta:
                                yield f'data: {json.dumps({"type": "text-delta", "id": part_info["id"], "delta": delta.content_delta})}\n\n'

                        elif isinstance(delta, ToolCallPartDelta):
                            if delta.args_delta:
                                args_str = delta.args_delta if isinstance(delta.args_delta, str) else json.dumps(delta.args_delta)
                                yield f'data: {json.dumps({"type": "tool-input-delta", "toolCallId": part_info["id"], "inputTextDelta": args_str})}\n\n'

                        elif isinstance(delta, ThinkingPartDelta):
                            if delta.content_delta:
                                yield f'data: {json.dumps({"type": "reasoning-delta", "id": part_info["id"], "delta": delta.content_delta})}\n\n'

                    # Handle FinalResultEvent
                    elif isinstance(event, FinalResultEvent):
                        # Close any active parts
                        for idx, part_info in active_parts.items():
                            if part_info["type"] == "text":
                                yield f'data: {json.dumps({"type": "text-end", "id": part_info["id"]})}\n\n'
                            elif part_info["type"] == "thinking":
                                yield f'data: {json.dumps({"type": "reasoning-end", "id": part_info["id"]})}\n\n'
                        active_parts.clear()

        elif Agent.is_call_tools_node(node):
            async with node.stream(agent_run.ctx) as stream:
                async for event in stream:
                    if isinstance(event, FunctionToolCallEvent):
                        # Tool input available
                        tool_call_id = event.part.tool_call_id
                        yield f'data: {json.dumps({"type": "tool-input-available", "toolCallId": tool_call_id, "toolName": event.part.tool_name, "input": event.part.args})}\n\n'

                    elif isinstance(event, FunctionToolResultEvent):
                        # Tool output available
                        tool_call_id = event.result.tool_call_id
                        content = event.result.content
                        yield f'data: {json.dumps({"type": "tool-output-available", "toolCallId": tool_call_id, "output": content})}\n\n'

    # Stream termination
    yield 'data: [DONE]\n\n'
```

## Index-Based Part Tracking

Events use an `index` field to track which part is being updated. This allows multiple parts to stream simultaneously:

```python
# Example: Model streams text AND a tool call at the same time
PartStartEvent(index=0, part=TextPart(content="I'll check that for you"))
PartDeltaEvent(index=0, delta=TextPartDelta(content_delta=" using the weather tool."))

PartStartEvent(index=1, part=ToolCallPart(tool_name="get_weather", ...))
PartDeltaEvent(index=1, delta=ToolCallPartDelta(args_delta='{"city":'))
PartDeltaEvent(index=1, delta=ToolCallPartDelta(args_delta='"Paris"}'))
```

**Best practice**: Maintain a dictionary mapping indices to accumulated parts:

```python
parts: dict[int, ModelResponsePart] = {}

async for event in stream:
    if isinstance(event, PartStartEvent):
        parts[event.index] = event.part
    elif isinstance(event, PartDeltaEvent):
        if event.index in parts:
            parts[event.index] = event.delta.apply(parts[event.index])
```

## Practical Example: Complete Vercel-Compatible Streaming

Here's a complete FastAPI example that streams to a web frontend using the Vercel protocol:

```python
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from pydantic_ai import Agent
from pydantic_ai.messages import (
    PartStartEvent, PartDeltaEvent,
    TextPart, TextPartDelta,
    ToolCallPart, ToolCallPartDelta,
    FunctionToolCallEvent, FunctionToolResultEvent
)
import json

app = FastAPI()

agent = Agent('openai:gpt-4o')

@agent.tool
def get_weather(city: str) -> dict:
    return {"city": city, "weather": "sunny", "temperature": 22}

@app.post("/api/chat")
async def chat(request: dict):
    prompt = request.get("prompt", "")

    async def generate():
        # Set the required header
        yield 'event: message\n'

        async with agent.iter(prompt) as agent_run:
            message_id = "msg_" + str(id(agent_run))

            # Message start
            yield f'data: {json.dumps({"type": "start", "messageId": message_id})}\n\n'

            async for node in agent_run:
                if Agent.is_model_request_node(node):
                    async with node.stream(agent_run.ctx) as stream:
                        text_id = None

                        async for event in stream:
                            if isinstance(event, PartStartEvent):
                                if isinstance(event.part, TextPart):
                                    text_id = f"{message_id}_text_{event.index}"
                                    yield f'data: {json.dumps({"type": "text-start", "id": text_id})}\n\n'

                                elif isinstance(event.part, ToolCallPart):
                                    tool_call_id = event.part.tool_call_id
                                    yield f'data: {json.dumps({"type": "tool-input-start", "toolCallId": tool_call_id, "toolName": event.part.tool_name})}\n\n'

                            elif isinstance(event, PartDeltaEvent):
                                if isinstance(event.delta, TextPartDelta) and text_id:
                                    yield f'data: {json.dumps({"type": "text-delta", "id": text_id, "delta": event.delta.content_delta})}\n\n'

                                elif isinstance(event.delta, ToolCallPartDelta):
                                    if event.delta.args_delta:
                                        args = event.delta.args_delta if isinstance(event.delta.args_delta, str) else json.dumps(event.delta.args_delta)
                                        yield f'data: {json.dumps({"type": "tool-input-delta", "toolCallId": event.delta.tool_call_id, "inputTextDelta": args})}\n\n'

                        if text_id:
                            yield f'data: {json.dumps({"type": "text-end", "id": text_id})}\n\n'

                elif Agent.is_call_tools_node(node):
                    async with node.stream(agent_run.ctx) as stream:
                        async for event in stream:
                            if isinstance(event, FunctionToolCallEvent):
                                yield f'data: {json.dumps({"type": "tool-input-available", "toolCallId": event.part.tool_call_id, "toolName": event.part.tool_name, "input": event.part.args})}\n\n'

                            elif isinstance(event, FunctionToolResultEvent):
                                yield f'data: {json.dumps({"type": "tool-output-available", "toolCallId": event.result.tool_call_id, "output": event.result.content})}\n\n'

            # Message finish
            yield f'data: {json.dumps({"type": "finish"})}\n\n'
            yield 'data: [DONE]\n\n'

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
            "x-vercel-ai-ui-message-stream": "v1"
        }
    )
```

## Key Differences from run_stream()

| Feature | `run_stream()` | `agent.iter()` |
|---------|----------------|----------------|
| **Works with tools** | ❌ `stream_text()` breaks | ✅ Full support |
| **Access to tool events** | ❌ Not available | ✅ FunctionToolCallEvent, FunctionToolResultEvent |
| **Graph node visibility** | ❌ Hidden | ✅ Explicit ModelRequestNode, CallToolsNode |
| **Delta access** | ✅ Via `stream_text(delta=True)` | ✅ Via PartDeltaEvent |
| **Thinking support** | ❌ Not accessible | ✅ ThinkingPartDelta |
| **Fine-grained control** | ❌ Limited | ✅ Full event access |

## Common Patterns

### Pattern 1: Text-Only Streaming (Simple)

```python
async with agent.iter(prompt) as agent_run:
    async for node in agent_run:
        if Agent.is_model_request_node(node):
            async with node.stream(agent_run.ctx) as stream:
                async for event in stream:
                    if isinstance(event, PartDeltaEvent):
                        if isinstance(event.delta, TextPartDelta):
                            print(event.delta.content_delta, end='', flush=True)
```

### Pattern 2: Show Tool Activity

```python
async with agent.iter(prompt) as agent_run:
    async for node in agent_run:
        if Agent.is_model_request_node(node):
            async with node.stream(agent_run.ctx) as stream:
                async for event in stream:
                    if isinstance(event, PartDeltaEvent):
                        if isinstance(event.delta, TextPartDelta):
                            yield {"type": "text", "delta": event.delta.content_delta}

        elif Agent.is_call_tools_node(node):
            async with node.stream(agent_run.ctx) as stream:
                async for event in stream:
                    if isinstance(event, FunctionToolCallEvent):
                        yield {"type": "tool_start", "name": event.part.tool_name}
                    elif isinstance(event, FunctionToolResultEvent):
                        yield {"type": "tool_result", "result": event.result.content}
```

### Pattern 3: Accumulate Full Response While Streaming

```python
from pydantic_ai.messages import ModelResponse

accumulated_parts = []

async with agent.iter(prompt) as agent_run:
    async for node in agent_run:
        if Agent.is_model_request_node(node):
            async with node.stream(agent_run.ctx) as stream:
                # Track parts by index
                parts_by_index: dict[int, ModelResponsePart] = {}

                async for event in stream:
                    if isinstance(event, PartStartEvent):
                        parts_by_index[event.index] = event.part
                        yield {"type": "start", "part": type(event.part).__name__}

                    elif isinstance(event, PartDeltaEvent):
                        if event.index in parts_by_index:
                            parts_by_index[event.index] = event.delta.apply(parts_by_index[event.index])

                        if isinstance(event.delta, TextPartDelta):
                            yield {"type": "text", "delta": event.delta.content_delta}

                # After streaming completes, we have all parts
                accumulated_parts = list(parts_by_index.values())

# Now you have the complete response
final_response = ModelResponse(parts=accumulated_parts)
```

## Summary

**Key Takeaways**:

1. **Three delta types**: `TextPartDelta`, `ToolCallPartDelta`, `ThinkingPartDelta`
2. **Two node types**: `ModelRequestNode` (model generating) and `CallToolsNode` (tools executing)
3. **Event streaming**: Use `node.stream()` to get a stream of events
4. **Index tracking**: Events use `index` to identify which part they update
5. **Vercel mapping**: Each delta type maps to specific Vercel protocol events
6. **Tool call streaming**: Tool name and arguments stream incrementally, just like text
7. **Thinking support**: Models with extended thinking stream their reasoning process

**When to use agent.iter()**:
- ✅ You have tools and need streaming
- ✅ You want to show tool execution progress to users
- ✅ You need fine-grained control over streaming events
- ✅ You're integrating with Vercel AI SDK or similar protocols
- ✅ You want to support models with extended thinking/reasoning

**agent.iter() is the recommended approach for any non-trivial streaming use case.**
